{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zf = zipfile.ZipFile('datasets/train_catvnoncat_h5.zip', mode='w')\n",
    "\n",
    "try:\n",
    "\n",
    "    zf.write('datasets/train_catvnoncat.h5')\n",
    "\n",
    "    zf.write('datasets/test_catvnoncat.h5')\n",
    "\n",
    "finally:\n",
    "\n",
    "    zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 17,  31,  56],\n",
       "         [ 22,  33,  59],\n",
       "         [ 25,  35,  62],\n",
       "         ..., \n",
       "         [  1,  28,  57],\n",
       "         [  1,  26,  56],\n",
       "         [  1,  22,  51]],\n",
       "\n",
       "        [[ 25,  36,  62],\n",
       "         [ 28,  38,  64],\n",
       "         [ 30,  40,  67],\n",
       "         ..., \n",
       "         [  1,  27,  56],\n",
       "         [  1,  25,  55],\n",
       "         [  2,  21,  51]],\n",
       "\n",
       "        [[ 32,  40,  67],\n",
       "         [ 34,  42,  69],\n",
       "         [ 35,  42,  70],\n",
       "         ..., \n",
       "         [  1,  25,  55],\n",
       "         [  0,  24,  54],\n",
       "         [  1,  21,  51]],\n",
       "\n",
       "        ..., \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ..., \n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ..., \n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ..., \n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]]],\n",
       "\n",
       "\n",
       "       [[[196, 192, 190],\n",
       "         [193, 186, 182],\n",
       "         [188, 179, 174],\n",
       "         ..., \n",
       "         [ 90, 142, 200],\n",
       "         [ 90, 142, 201],\n",
       "         [ 90, 142, 201]],\n",
       "\n",
       "        [[230, 229, 229],\n",
       "         [204, 199, 197],\n",
       "         [193, 186, 181],\n",
       "         ..., \n",
       "         [ 91, 143, 201],\n",
       "         [ 91, 143, 201],\n",
       "         [ 91, 143, 201]],\n",
       "\n",
       "        [[232, 225, 224],\n",
       "         [235, 234, 234],\n",
       "         [208, 205, 202],\n",
       "         ..., \n",
       "         [ 91, 144, 202],\n",
       "         [ 91, 144, 202],\n",
       "         [ 92, 144, 202]],\n",
       "\n",
       "        ..., \n",
       "        [[ 18,  17,  15],\n",
       "         [ 14,  14,  13],\n",
       "         [ 29,  29,  32],\n",
       "         ..., \n",
       "         [ 83,  81,  81],\n",
       "         [ 84,  82,  83],\n",
       "         [ 82,  81,  82]],\n",
       "\n",
       "        [[ 22,  20,  18],\n",
       "         [ 16,  15,  14],\n",
       "         [ 25,  24,  24],\n",
       "         ..., \n",
       "         [ 82,  80,  80],\n",
       "         [ 83,  81,  82],\n",
       "         [ 82,  81,  81]],\n",
       "\n",
       "        [[ 45,  43,  39],\n",
       "         [ 61,  59,  54],\n",
       "         [ 81,  78,  74],\n",
       "         ..., \n",
       "         [ 83,  82,  81],\n",
       "         [ 84,  82,  82],\n",
       "         [ 82,  80,  81]]],\n",
       "\n",
       "\n",
       "       [[[ 82,  71,  68],\n",
       "         [ 89,  83,  83],\n",
       "         [100,  98, 104],\n",
       "         ..., \n",
       "         [131, 132, 137],\n",
       "         [126, 124, 124],\n",
       "         [105,  97,  95]],\n",
       "\n",
       "        [[ 95,  91,  97],\n",
       "         [104, 104, 113],\n",
       "         [110, 115, 126],\n",
       "         ..., \n",
       "         [135, 134, 135],\n",
       "         [127, 122, 119],\n",
       "         [111, 105, 103]],\n",
       "\n",
       "        [[ 94,  85,  83],\n",
       "         [ 97,  89,  90],\n",
       "         [110, 109, 115],\n",
       "         ..., \n",
       "         [136, 134, 131],\n",
       "         [127, 120, 117],\n",
       "         [116, 108, 104]],\n",
       "\n",
       "        ..., \n",
       "        [[ 96, 116, 131],\n",
       "         [ 97, 115, 130],\n",
       "         [103, 123, 139],\n",
       "         ..., \n",
       "         [152, 155, 157],\n",
       "         [146, 149, 152],\n",
       "         [130, 133, 134]],\n",
       "\n",
       "        [[ 90, 108, 123],\n",
       "         [ 92, 108, 121],\n",
       "         [100, 119, 134],\n",
       "         ..., \n",
       "         [150, 152, 155],\n",
       "         [144, 146, 147],\n",
       "         [134, 135, 134]],\n",
       "\n",
       "        [[ 86, 102, 116],\n",
       "         [ 87, 103, 115],\n",
       "         [ 94, 114, 127],\n",
       "         ..., \n",
       "         [154, 156, 160],\n",
       "         [146, 148, 152],\n",
       "         [138, 141, 142]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[143, 155, 165],\n",
       "         [184, 190, 198],\n",
       "         [142, 149, 155],\n",
       "         ..., \n",
       "         [ 99,  92, 102],\n",
       "         [120,  98, 102],\n",
       "         [100,  84,  95]],\n",
       "\n",
       "        [[151, 149, 139],\n",
       "         [173, 179, 185],\n",
       "         [105, 135, 141],\n",
       "         ..., \n",
       "         [ 91,  87,  99],\n",
       "         [119,  99, 104],\n",
       "         [120,  95, 101]],\n",
       "\n",
       "        [[204, 190, 185],\n",
       "         [180, 185, 195],\n",
       "         [117, 155, 177],\n",
       "         ..., \n",
       "         [ 96,  88, 101],\n",
       "         [125, 103, 110],\n",
       "         [120, 100, 110]],\n",
       "\n",
       "        ..., \n",
       "        [[ 41,  80, 116],\n",
       "         [ 41,  80, 116],\n",
       "         [ 41,  78, 115],\n",
       "         ..., \n",
       "         [ 63,  75,  98],\n",
       "         [ 60,  72,  98],\n",
       "         [ 60,  70,  96]],\n",
       "\n",
       "        [[ 71,  90, 121],\n",
       "         [ 73,  91, 123],\n",
       "         [ 74,  91, 124],\n",
       "         ..., \n",
       "         [ 79, 101, 142],\n",
       "         [ 80, 100, 140],\n",
       "         [ 82, 101, 139]],\n",
       "\n",
       "        [[ 71,  88, 122],\n",
       "         [ 73,  92, 128],\n",
       "         [ 76,  95, 131],\n",
       "         ..., \n",
       "         [ 81, 106, 150],\n",
       "         [ 85, 108, 151],\n",
       "         [ 85, 107, 149]]],\n",
       "\n",
       "\n",
       "       [[[ 22,  24,  23],\n",
       "         [ 23,  25,  24],\n",
       "         [ 24,  26,  25],\n",
       "         ..., \n",
       "         [ 24,  29,  25],\n",
       "         [ 23,  25,  22],\n",
       "         [ 20,  22,  21]],\n",
       "\n",
       "        [[ 22,  24,  23],\n",
       "         [ 23,  25,  24],\n",
       "         [ 23,  26,  25],\n",
       "         ..., \n",
       "         [ 22,  28,  23],\n",
       "         [ 20,  23,  22],\n",
       "         [ 19,  21,  21]],\n",
       "\n",
       "        [[ 22,  24,  22],\n",
       "         [ 23,  25,  24],\n",
       "         [ 23,  26,  25],\n",
       "         ..., \n",
       "         [ 23,  27,  23],\n",
       "         [ 20,  23,  21],\n",
       "         [ 18,  20,  19]],\n",
       "\n",
       "        ..., \n",
       "        [[  8,   5,   0],\n",
       "         [  9,   6,   1],\n",
       "         [  9,   6,   1],\n",
       "         ..., \n",
       "         [  4,   5,   0],\n",
       "         [  5,   4,   0],\n",
       "         [  4,   5,   0]],\n",
       "\n",
       "        [[  7,   5,   0],\n",
       "         [  8,   5,   1],\n",
       "         [  9,   6,   1],\n",
       "         ..., \n",
       "         [  4,   5,   0],\n",
       "         [  4,   5,   0],\n",
       "         [  4,   5,   0]],\n",
       "\n",
       "        [[  7,   5,   0],\n",
       "         [  8,   5,   0],\n",
       "         [  9,   6,   1],\n",
       "         ..., \n",
       "         [  4,   5,   0],\n",
       "         [  4,   5,   0],\n",
       "         [  4,   5,   0]]],\n",
       "\n",
       "\n",
       "       [[[  8,  28,  53],\n",
       "         [ 14,  33,  58],\n",
       "         [ 19,  35,  61],\n",
       "         ..., \n",
       "         [ 11,  16,  35],\n",
       "         [ 10,  16,  35],\n",
       "         [  9,  14,  32]],\n",
       "\n",
       "        [[ 15,  31,  57],\n",
       "         [ 15,  32,  58],\n",
       "         [ 18,  34,  60],\n",
       "         ..., \n",
       "         [ 13,  17,  35],\n",
       "         [ 13,  17,  35],\n",
       "         [ 13,  16,  35]],\n",
       "\n",
       "        [[ 20,  35,  61],\n",
       "         [ 19,  33,  59],\n",
       "         [ 20,  33,  59],\n",
       "         ..., \n",
       "         [ 16,  17,  35],\n",
       "         [ 16,  18,  35],\n",
       "         [ 15,  17,  35]],\n",
       "\n",
       "        ..., \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ..., \n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ..., \n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       "\n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ..., \n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]]]], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 64, 64, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_orig.shape\n",
    "# 209 rows \n",
    "# 64,64,3 signifies each image is of shape num_px, num_px, 3 where 3 signifies 3 channels. Each image is square(height= num_px and width= num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13724943],\n",
       "       [ 1.04217794],\n",
       "       [ 0.57738677],\n",
       "       [ 1.19174757],\n",
       "       [ 1.13005384]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= np.random.randn(5,1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 64, 64, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_x_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #Exercise: Find the values for:\n",
    "- m_train (number of training examples)\n",
    "- m_test (number of test examples)\n",
    "- num_px (= height = width of a training image)\n",
    "Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_train=train_set_x_orig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of training examples: m_train = \" + str(m_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test examples: m_test = 50\n"
     ]
    }
   ],
   "source": [
    "m_test=test_set_x_orig.shape[0]\n",
    "print (\"Number of test examples: m_test = \" + str(m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px  ∗∗  num_px  ∗∗  3, 1).\n",
    "\n",
    "\n",
    "---A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗∗ c ∗∗ d, a) is to use:\n",
    "#### X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 50)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "test_set_x_flatten.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06666667,  0.76862745,  0.32156863, ...,  0.56078431,\n",
       "         0.08627451,  0.03137255],\n",
       "       [ 0.12156863,  0.75294118,  0.27843137, ...,  0.60784314,\n",
       "         0.09411765,  0.10980392],\n",
       "       [ 0.21960784,  0.74509804,  0.26666667, ...,  0.64705882,\n",
       "         0.09019608,  0.20784314],\n",
       "       ..., \n",
       "       [ 0.        ,  0.32156863,  0.54117647, ...,  0.33333333,\n",
       "         0.01568627,  0.        ],\n",
       "       [ 0.        ,  0.31372549,  0.55294118, ...,  0.41960784,\n",
       "         0.01960784,  0.        ],\n",
       "       [ 0.        ,  0.31764706,  0.55686275, ...,  0.58431373,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61960784,  0.45098039,  1.        , ...,  0.16078431,\n",
       "         0.07058824,  0.52156863],\n",
       "       [ 0.40784314,  0.43137255,  0.99215686, ...,  0.18431373,\n",
       "         0.07058824,  0.63921569],\n",
       "       [ 0.3254902 ,  0.43529412,  0.99607843, ...,  0.32941176,\n",
       "         0.0627451 ,  0.29411765],\n",
       "       ..., \n",
       "       [ 0.67843137,  0.67058824,  0.52156863, ...,  0.71764706,\n",
       "         0.56470588,  0.01960784],\n",
       "       [ 0.50196078,  0.69019608,  0.39607843, ...,  0.55294118,\n",
       "         0.5372549 ,  0.08627451],\n",
       "       [ 0.43137255,  0.72941176,  0.4745098 , ...,  0.45490196,\n",
       "         0.42352941,  0.01960784]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Architecture of the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building the parts of our algorithm\n",
    "The main steps for building a Neural Network are:\n",
    "### Define the model structure (such as number of input features)\n",
    "### Initialize the model's parameters\n",
    "## Loop:\n",
    "### Calculate current loss (forward propagation)\n",
    "### Calculate current gradient (backward propagation)\n",
    "### Update parameters (gradient descent)\n",
    "You often build 1-3 separately and integrate them into one function we call model().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Helper function- Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s=None\n",
    "    s=1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "# initialization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b=0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# forward and backward propogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = None # compute activation\n",
    "    A= sigmoid(np.dot(w.T,X)+b)\n",
    "    cost = None   # compute cost\n",
    "    cost=(-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dw = None\n",
    "    db = None\n",
    "    dw= (np.dot(X,np.transpose(A-Y)))/m\n",
    "    db = (np.sum(A-Y))/m\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, you want to update the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        ##grads, cost = None\n",
    "        #A = sigmoid(np.dot(w.T,X) + b) \n",
    "        ##cost = np.mean(-(Y * np.log(A) + (1-Y) * np.log(1-A)),axis=1) \n",
    "        #A = sigmoid(np.dot(w.T,X) + b)  \n",
    "        grads,cost=propagate(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        #w = None\n",
    "        #b = None\n",
    "        #dw = np.mean(np.sum(np.dot(X, (A - Y).T), axis=1, keepdims=True))\n",
    "        #db = np.mean(np.sum(A - Y))\n",
    "        w = (w - (learning_rate * dw))\n",
    "        b = (b - (learning_rate * db))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:** The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There is two steps to computing predictions:\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    " \n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = None\n",
    "    A=sigmoid(np.dot(w.T, X)+b)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0][i] > 0.5:\n",
    "            Y_prediction[0][i] = 1\n",
    "        else:\n",
    "            Y_prediction[0][i] = 0\n",
    "        pass\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all function into Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    #w, b = None\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    \n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    #parameters, grads, costs = None\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate , print_cost = False)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    #Y_prediction_test = None\n",
    "    #Y_prediction_train = None\n",
    "    Y_prediction_train=predict(w, b, X_train)\n",
    "    Y_prediction_test=predict(w, b, X_test)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
